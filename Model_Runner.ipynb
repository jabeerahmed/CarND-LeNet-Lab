{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageClassifier Loaded!!!\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport ImageClassifier\n",
    "%aimport model_runner\n",
    "%aimport ConvNet\n",
    "%aimport Timer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ImageClassifier import ImageClassifier as IC\n",
    "from ImageClassifier import ModelTrainer as MT\n",
    "from Timer import Timer\n",
    "from model_runner import *\n",
    "import ConvNet as CNN\n",
    "from matplotlib import pyplot\n",
    "\n",
    "def basicNorm(img):\n",
    "    shape = img.shape\n",
    "    dst = np.float32(img)\n",
    "    if (len(shape) == 2):\n",
    "            p  = dst\n",
    "            p -= p.min()\n",
    "            p /= p.max()        \n",
    "    else:\n",
    "        for i in range(shape[2]): \n",
    "            p  = dst[:, :, i]\n",
    "            p -= p.min()\n",
    "            p /= p.max()\n",
    "\n",
    "    return dst\n",
    "            \n",
    "def visualize(data, setname='train'):\n",
    "    \n",
    "    dataset = data.dmap[setname]\n",
    "    signnames = data.signs\n",
    "    col_width = max(len(name) for name in signnames)\n",
    "    class_counts = data.get_data_dist(setname)\n",
    "    \n",
    "    for c in range(len(signnames)):\n",
    "        X_train = dataset[c]\n",
    "        c_count = len(X_train)\n",
    "        c_index = 0\n",
    "        \n",
    "        print(\"Class %i: %-*s  %s samples\" % (c, col_width, signnames[c], str(c_count)))\n",
    "        fig = pyplot.figure(figsize = (6, 1))\n",
    "        fig.subplots_adjust(left = 0, right = 1, bottom = 0, top = 1, hspace = 0.05, wspace = 0.05)\n",
    "        random_indices = random.sample(range(c_index, c_index + c_count), 10)\n",
    "        for i in range(10):\n",
    "            axis = fig.add_subplot(1, 10, i + 1, xticks=[], yticks=[])\n",
    "            axis.imshow(basicNorm(X_train[random_indices[i]]))\n",
    "                \n",
    "        pyplot.show()\n",
    "        print(\"--------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "    pyplot.bar( np.arange( 43 ), class_counts, align='center' )\n",
    "    pyplot.xlabel('Class')\n",
    "    pyplot.ylabel('Number of training examples')\n",
    "    pyplot.xlim([-1, 43])\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load + Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "def load(dirpath='traffic-signs-data'):\n",
    "    org  = IC(data_dir=dirpath)\n",
    "    data = DataModifier(org)\n",
    "    return org, data\n",
    "\n",
    "def saveData(org, dirpath):\n",
    "    dirpath = Utils.find_an_empty_dir(dirpath)\n",
    "    print(\"Saving data to: \" + dirpath)\n",
    "    if (os.path.exists(dirpath) == False): os.makedirs(dirpath)\n",
    "    with open(os.path.join(dirpath, 'train.p'), \"wb\") as fp:\n",
    "        pickle.dump(org.train, fp)\n",
    "    with open(os.path.join(dirpath, 'test.p'), \"wb\" ) as fp:\n",
    "        pickle.dump(org.test, fp)\n",
    "    with open(os.path.join(dirpath, 'valid.p'), \"wb\") as fp:\n",
    "        pickle.dump(org.valid, fp)\n",
    "\n",
    "def augmentData(org=None, data=None, save_dir=None, num_total=1000):\n",
    "#     augmentDataSet(data.train)\n",
    "#     augmentDataSet(data.test)\n",
    "    n_train,n_test,n_valid = num_total, np.int(num_total*0.3), np.int(num_total*0.1)\n",
    "    augmentDatasetPerspective(data.train, num_total=n_train, sz=26, delta=3, t_rng=range(-3, 3), perspTrans=True)\n",
    "    augmentDatasetPerspective(data.test,  num_total=n_test , sz=26, delta=3, t_rng=range(-3, 3), perspTrans=True)\n",
    "    augmentDatasetPerspective(data.valid, num_total=n_valid, sz=26, delta=3, t_rng=range(-3, 3), perspTrans=True)\n",
    "    DataModifier.updateDataSet(org, data.train, org.train)\n",
    "    DataModifier.updateDataSet(org, data.test,  org.test)\n",
    "    DataModifier.updateDataSet(org, data.valid, org.valid)    \n",
    "    if (save_dir is not None): saveData(org, save_dir)\n",
    "    return org, data\n",
    "\n",
    "# for n in range(3):\n",
    "#     for i in [1, 2, 3]:\n",
    "#         org, data = Timer.run(load)\n",
    "#         org, data = Timer.run(augmentData, args={'org': org, 'data': data, 'num_total': i*1000, 'save_dir':'aug{:1d}K_sz26_d3_rng3_persp'.format(i)})\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "      | Train  | Test   | Valid  |\n",
      "---------------------------------\n",
      " (#)  | 129000 |  38700 |  12900 |\n",
      " (%)  |   1.00 |   0.30 |   0.10 |\n",
      "---------------------------------\n",
      "Number Classes  : 43\n",
      "Image Dimensions: (32, 32, 3)\n",
      "\n",
      "---------------------------------------------------------------\n",
      "elapsed time: 2.668 secs\n",
      "---------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "data_path = 'data/aug3K_sz26_d3_rng3_persp/000'\n",
    "org, data = Timer.run(load, args={'dirpath': data_path})\n",
    "# org, data = Timer.run(load)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reset / Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# org.reset_data()\n",
    "# visualize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------\n",
      "elapsed time: 7.170 secs\n",
      "---------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageClassifier Loaded!!!\n",
      "---------------------------------------------------------------\n",
      "elapsed time: 7.205 secs\n",
      "---------------------------------------------------------------\n",
      "Training...\n",
      "\n",
      "EPOCH  1 ...\n",
      "Validation Accuracy = 0.673\n",
      "\n",
      "EPOCH  2 ...\n",
      "Validation Accuracy = 0.768\n",
      "\n",
      "EPOCH  3 ...\n",
      "Validation Accuracy = 0.804\n",
      "\n",
      "EPOCH  4 ...\n",
      "Validation Accuracy = 0.782\n",
      "\n",
      "EPOCH  5 ...\n",
      "Validation Accuracy = 0.833\n",
      "\n",
      "EPOCH  6 ...\n",
      "Validation Accuracy = 0.840\n",
      "\n",
      "EPOCH  7 ...\n",
      "Validation Accuracy = 0.872\n",
      "\n",
      "EPOCH  8 ...\n",
      "Validation Accuracy = 0.865\n",
      "\n",
      "EPOCH  9 ...\n",
      "Validation Accuracy = 0.829\n",
      "\n",
      "EPOCH 10 ...\n",
      "Validation Accuracy = 0.860\n",
      "\n",
      "EPOCH 11 ...\n",
      "Validation Accuracy = 0.850\n",
      "\n",
      "EPOCH 12 ...\n",
      "Validation Accuracy = 0.874\n",
      "\n",
      "EPOCH 13 ...\n",
      "Validation Accuracy = 0.880\n",
      "\n",
      "EPOCH 14 ...\n",
      "Validation Accuracy = 0.841\n",
      "\n",
      "EPOCH 15 ...\n",
      "Validation Accuracy = 0.881\n",
      "\n",
      "EPOCH 16 ...\n",
      "Validation Accuracy = 0.880\n",
      "\n",
      "EPOCH 17 ...\n",
      "Validation Accuracy = 0.883\n",
      "\n",
      "EPOCH 18 ...\n",
      "Validation Accuracy = 0.896\n",
      "\n",
      "EPOCH 19 ...\n",
      "Validation Accuracy = 0.885\n",
      "\n",
      "EPOCH 20 ...\n",
      "Validation Accuracy = 0.877\n",
      "\n",
      "EPOCH 21 ...\n",
      "Validation Accuracy = 0.886\n",
      "\n",
      "EPOCH 22 ...\n",
      "Validation Accuracy = 0.867\n",
      "\n",
      "EPOCH 23 ...\n",
      "Validation Accuracy = 0.892\n",
      "\n",
      "EPOCH 24 ...\n",
      "Validation Accuracy = 0.884\n",
      "\n",
      "EPOCH 25 ...\n",
      "Validation Accuracy = 0.889\n",
      "\n",
      "Model saved : ./009/LeNetWithDropOut\n",
      "---------------------------------------------------------------\n",
      "elapsed time: 189.648 secs\n",
      "---------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def preprocess(org=None, data=None, pre_ops=[]):\n",
    "    if (len(pre_ops) > 0): org.preprocess_all(pre_ops)\n",
    "    return org, data\n",
    "\n",
    "\n",
    "def Center_1to1(src, i, dst):\n",
    "    dst[i] = (src[i] - 128.0)/128.0\n",
    "\n",
    "kEPOCHS, kBATCH_SIZE, kRATE, kNETWORK, kNETWORK_ARGS = 'EPOCHS', 'BATCH_SIZE', 'RATE', 'NETWORK', 'NETWORK_ARGS'\n",
    "\n",
    "def train(D=None, EPOCHS=10, BATCH_SIZE=128, RATE=0.001, NETWORK=MT.LeNet, NETWORK_ARGS={}):\n",
    "    if D is None: raise Exception('D in None')\n",
    "    T = MT(D, EPOCHS=EPOCHS, BATCH_SIZE=BATCH_SIZE, rate=RATE, network=NETWORK, network_args=NETWORK_ARGS)\n",
    "    T.train()\n",
    "    return D, T\n",
    "\n",
    "# pre_ops = [Center_1to1]\n",
    "pre_ops = [IC.Norm, IC.ZMean]\n",
    "org, data = Timer.run(preprocess,  args={'org': org, 'data': data, 'pre_ops':pre_ops})\n",
    "\n",
    "# , 'dropouts':{0: 0.9, 1:0.6}\n",
    "args = {'D': org, 'EPOCHS': 25, 'BATCH_SIZE': 64, 'RATE': 0.0001, 'NETWORK': MT.LeNetWithDropOut, 'NETWORK_ARGS': {'sigma':0.001}} \n",
    "\n",
    "org, modl = Timer.run(train, args=args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "visualize(org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
